{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dp-93/RAG-PDF-chatbot/blob/main/Chat_with_Your_PDF_(RAG).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain_google_genai PyPDF2 faiss-cpu langchain-community"
      ],
      "metadata": {
        "id": "uINHLFvj83bH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# --- Configuration ---\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBHQAZt0KaG4QSJ0pLUapMoIInKbjDWrw8\" # Make sure your key is here\n",
        "PDF_FILE_PATH = \"Java 8 feature.pdf\" # Make sure your filename is here\n",
        "\n",
        "# --- PDF Processing and Text Extraction ---\n",
        "def get_pdf_text(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        pdf_reader = PdfReader(pdf_path)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{pdf_path}' was not found. Please upload your PDF and check the filename.\")\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "# --- Text Chunking ---\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# --- Embedding and Vector Store Creation ---\n",
        "def get_vector_store(text_chunks):\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "    print(\"Vector store created and saved.\")\n",
        "\n",
        "# --- Conversational Chain Setup ---\n",
        "def get_conversational_chain():\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context. If the answer is not in\n",
        "    the provided context, just say, \"The answer is not available in the context\". Do not provide a wrong answer.\\n\\n\n",
        "    Context:\\n{context}\\n\n",
        "    Question:\\n{question}\\n\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    # *** FIX #1: Updated the model name ***\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.3)\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "    return chain\n",
        "\n",
        "# --- Main Application Logic ---\n",
        "def user_input(user_question):\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    try:\n",
        "        new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading vector store: {e}\")\n",
        "        return\n",
        "\n",
        "    docs = new_db.similarity_search(user_question)\n",
        "    chain = get_conversational_chain()\n",
        "\n",
        "    # *** FIX #2: Updated to use the new .invoke() method ***\n",
        "    response = chain.invoke({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\n",
        "\n",
        "    print(\"\\n--- Answer ---\")\n",
        "    print(response[\"output_text\"])\n",
        "\n",
        "\n",
        "# --- Running the RAG Pipeline ---\n",
        "if 'rag_initialized' not in locals():\n",
        "    raw_text = get_pdf_text(PDF_FILE_PATH)\n",
        "    if raw_text:\n",
        "        text_chunks = get_text_chunks(raw_text)\n",
        "        get_vector_store(text_chunks)\n",
        "        print(\"\\nPDF processed successfully. You can now ask questions in a new cell.\")\n",
        "        rag_initialized = True\n",
        "\n"
      ],
      "metadata": {
        "id": "Zd8X3GPl9Ypj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input(\"What is the main conclusion of this document?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ7SOprzBXu7",
        "outputId": "a8fdcf9c-c32e-4732-9972-bc8c18d5c7fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Answer ---\n",
            "The document comprehensively details Java 8 features, focusing on functional programming concepts.  It explains default and static methods in interfaces, functional interfaces (their purpose and examples like `Function`, `Predicate`, `Consumer`, and `Supplier`), lambda expressions, method references, the Stream API, and higher-order functions.  The main conclusion is that Java 8 significantly enhanced Java's capabilities by incorporating functional programming paradigms, leading to more concise, readable, and maintainable code through features like lambda expressions, streams, and functional interfaces.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Install all necessary libraries ---\n",
        "!pip install -q gradio langchain langchain_google_genai PyPDF2 faiss-cpu langchain-community\n",
        "\n",
        "import os\n",
        "import gradio as gr\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "# IMPORTANT: Add your Google API Key here.\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBHQAZt0KaG4QSJ0pLUapMoIInKbjDWrw8\"\n",
        "\n",
        "\n",
        "# --- 3. Core RAG Functions ---\n",
        "\n",
        "def process_pdf_and_create_db(pdf_file):\n",
        "    if pdf_file is None:\n",
        "        return None, gr.update(value=\"Please upload a PDF file first.\", visible=True), gr.update(visible=False)\n",
        "\n",
        "    try:\n",
        "        pdf_reader = PdfReader(pdf_file.name)\n",
        "        text = \"\".join(page.extract_text() for page in pdf_reader.pages)\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "        chunks = text_splitter.split_text(text)\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
        "\n",
        "        # This returns the success status and updates the UI visibility\n",
        "        return vector_store, gr.update(value=\"PDF processed successfully! Ready to chat.\", visible=True), gr.update(visible=True)\n",
        "    except Exception as e:\n",
        "        return None, gr.update(value=f\"Error processing PDF: {e}\", visible=True), gr.update(visible=False)\n",
        "\n",
        "\n",
        "def get_conversational_chain():\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context. If the answer is not in\n",
        "    the provided context, just say, \"The answer is not available in the context\". Do not provide a wrong answer.\\n\\n\n",
        "    Context:\\n{context}\\n\n",
        "    Question:\\n{question}\\n\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.3)\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "    return load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "\n",
        "# --- 4. Gradio Chatbot Logic ---\n",
        "\n",
        "def user(user_message, history):\n",
        "    return \"\", history + [[user_message, None]]\n",
        "\n",
        "def bot(history, vector_store):\n",
        "    if vector_store is None:\n",
        "        history[-1][1] = \"Error: PDF not processed. Please upload a PDF in Step 1.\"\n",
        "        return history\n",
        "\n",
        "    user_question = history[-1][0]\n",
        "\n",
        "    try:\n",
        "        docs = vector_store.similarity_search(user_question)\n",
        "        chain = get_conversational_chain()\n",
        "        response = chain.invoke({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\n",
        "        bot_message = response[\"output_text\"]\n",
        "    except Exception as e:\n",
        "        bot_message = f\"An error occurred: {e}\"\n",
        "\n",
        "    history[-1][1] = bot_message\n",
        "    return history\n",
        "\n",
        "\n",
        "# --- 5. Gradio Interface Definition ---\n",
        "\n",
        "with gr.Blocks(title=\"Chat with Your PDF\", theme=gr.themes.Soft()) as demo:\n",
        "    db_state = gr.State(None)\n",
        "\n",
        "    gr.Markdown(\"# ðŸ’¬ Chat with Your PDF\")\n",
        "    gr.Markdown(\"This app allows you to chat with a PDF document using a powerful AI model.\")\n",
        "\n",
        "    with gr.Accordion(\"Step 1: Upload Your PDF\", open=True):\n",
        "        pdf_input = gr.File(label=\"Upload your PDF\", type=\"filepath\")\n",
        "        status_output = gr.Textbox(label=\"Processing Status\", interactive=False, visible=False)\n",
        "\n",
        "    # *** THE FIX IS HERE: Changed gr.Box to gr.Group ***\n",
        "    with gr.Group(visible=False) as chat_box: # This entire section is hidden initially\n",
        "        gr.Markdown(\"### Step 2: Ask Questions\")\n",
        "        chatbot = gr.Chatbot(label=\"Conversation\")\n",
        "        with gr.Row():\n",
        "            msg = gr.Textbox(label=\"Your Question\", placeholder=\"Ask a question about the PDF...\", scale=4)\n",
        "            submit_btn = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "\n",
        "    # When a PDF is uploaded, process it, update the status, and show/hide the chat box\n",
        "    pdf_input.upload(\n",
        "        process_pdf_and_create_db,\n",
        "        inputs=[pdf_input],\n",
        "        outputs=[db_state, status_output, chat_box]\n",
        "    )\n",
        "\n",
        "    # Handle chat submission\n",
        "    submit_btn.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "        bot, [chatbot, db_state], chatbot\n",
        "    )\n",
        "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "        bot, [chatbot, db_state], chatbot\n",
        "    )\n",
        "\n",
        "# Launch the app!\n",
        "demo.launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "S_iFEbnCDOtM",
        "outputId": "6deb1a98-4450-4479-96a5-e357dc9298ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1689594412.py:93: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Conversation\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f9951983112bed1956.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f9951983112bed1956.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}